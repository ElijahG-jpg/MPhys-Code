{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"09eaa3ff","cell_type":"code","source":"# Base code from stack exchange https://stackoverflow.com/questions/78088764/finding-the-coefficient-a-of-a-linear-equation-of-the-form-y-ax\n# Formed the class and training loop for me, I inputted data and edited code to remove errors\n\nimport torch\nfrom torch.optim import Adam\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using Device: {device}\")\n\nclass FindParameter(torch.nn.Module):\n    def __init__(self):\n        super(FindParameter, self).__init__()\n        self.a = torch.nn.Parameter(data=torch.tensor([1.0], dtype=torch.float32, requires_grad=True))\n\n    def forward(self, input):\n        return self.a * input\n\n\na = 3.5\nX = torch.randint(1, 10, (10,), dtype=torch.float32).to(device)\nY = a * X\n\nprint(f\"{X = }\")\nprint(f\"{Y = }\")\n\nmodel = FindParameter().to(device)\nloss_f = torch.nn.MSELoss()\noptimizer = Adam(model.parameters(), lr=1e-2)\n\nepochs = 1000\n\nfor epoch in range(epochs):\n    optimizer.zero_grad()\n    y_pred = model(X)\n\n    loss = loss_f(y_pred, Y)\n    loss.backward()\n    optimizer.step()\n\n    if epoch % 100 == 0:\n        print(f\"Epoch {epoch}, Loss: {loss.item()}, a: {model.a.item()}\")\n\nprint(loss.item())\nprint(model.a.item())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:39:34.863652Z","iopub.execute_input":"2025-06-05T15:39:34.864310Z","iopub.status.idle":"2025-06-05T15:39:35.542252Z","shell.execute_reply.started":"2025-06-05T15:39:34.864287Z","shell.execute_reply":"2025-06-05T15:39:35.541435Z"}},"outputs":[{"name":"stdout","text":"Using Device: cuda\nX = tensor([1., 2., 7., 1., 5., 7., 5., 6., 8., 4.], device='cuda:0')\nY = tensor([ 3.5000,  7.0000, 24.5000,  3.5000, 17.5000, 24.5000, 17.5000, 21.0000,\n        28.0000, 14.0000], device='cuda:0')\nEpoch 0, Loss: 168.75, a: 1.0099999904632568\nEpoch 100, Loss: 67.1010513305664, a: 1.9316694736480713\nEpoch 200, Loss: 21.162002563476562, a: 2.620351552963257\nEpoch 300, Loss: 5.10443115234375, a: 3.0685925483703613\nEpoch 400, Loss: 0.9177355170249939, a: 3.317350387573242\nEpoch 500, Loss: 0.12161264568567276, a: 3.4336135387420654\nEpoch 600, Loss: 0.011808179318904877, a: 3.47934627532959\nEpoch 700, Loss: 0.0008338458137586713, a: 3.494520425796509\nEpoch 800, Loss: 4.228938996675424e-05, a: 3.4987680912017822\nEpoch 900, Loss: 1.519915599601518e-06, a: 3.4997668266296387\n3.988547092603767e-08\n3.499962329864502\n","output_type":"stream"}],"execution_count":27},{"id":"83446c2b","cell_type":"code","source":"list(model.parameters())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T14:57:29.569140Z","iopub.execute_input":"2025-06-05T14:57:29.570125Z","iopub.status.idle":"2025-06-05T14:57:29.576950Z","shell.execute_reply.started":"2025-06-05T14:57:29.570088Z","shell.execute_reply":"2025-06-05T14:57:29.576233Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"[Parameter containing:\n tensor([3.5000], device='cuda:0', requires_grad=True)]"},"metadata":{}}],"execution_count":2},{"id":"1f5779fc-b057-4bc8-a74f-e62e8cdab291","cell_type":"code","source":"torch.randint(1, 10, (20,), dtype=torch.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:07:33.336874Z","iopub.execute_input":"2025-06-05T15:07:33.337589Z","iopub.status.idle":"2025-06-05T15:07:33.343085Z","shell.execute_reply.started":"2025-06-05T15:07:33.337567Z","shell.execute_reply":"2025-06-05T15:07:33.342428Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"tensor([8., 2., 4., 6., 7., 1., 3., 7., 4., 7., 9., 1., 3., 5., 4., 4., 5., 7.,\n        1., 3.])"},"metadata":{}}],"execution_count":12},{"id":"5eb8afdb","cell_type":"code","source":"class linear_test(torch.nn.Module):\n    def __init__(self):\n        super(linear_test,self).__init__()\n        # Parameters\n        self.m = torch.nn.Parameter(data=torch.tensor([1.0], dtype=torch.float32, requires_grad=True))\n        self.c = torch.nn.Parameter(data=torch.tensor([1.0], dtype=torch.float32, requires_grad=True))\n\n    def forward(self,input):\n        return self.m * input + self.c\n\n# Form training data\nm = 10\nc = 12\nX = torch.randint(1, 10, (50,), dtype=torch.float32).to(device)\nY = m*X + c\n\nprint(f\"{X = }\")\nprint(f\"{Y = }\")\n\nmodel = linear_test().to(device)\nloss_f = torch.nn.MSELoss()\noptimizer = Adam(model.parameters(), lr=1e-2)\n\nepochs = 10000\n\nfor epoch in range(epochs):\n    optimizer.zero_grad()\n    y_pred = model(X)\n\n    loss = loss_f(y_pred, Y)\n    loss.backward()\n    optimizer.step()\n\n    if epoch % 1000 == 0:\n        print(f\"Epoch {epoch}, Loss: {loss.item()}, m: {model.m.item()}, c: {model.c.item()}\")\n\nprint(f\"Final Loss: {loss.item()}\")\nprint(f\"Final m: {model.m.item()}\")\nprint(f\"Final c: {model.c.item()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:09:50.339704Z","iopub.execute_input":"2025-06-05T15:09:50.340369Z","iopub.status.idle":"2025-06-05T15:09:57.093558Z","shell.execute_reply.started":"2025-06-05T15:09:50.340347Z","shell.execute_reply":"2025-06-05T15:09:57.092967Z"}},"outputs":[{"name":"stdout","text":"X = tensor([7., 7., 4., 2., 4., 3., 7., 9., 5., 7., 4., 1., 4., 2., 3., 6., 9., 3.,\n        2., 3., 3., 4., 4., 1., 2., 5., 6., 4., 1., 9., 6., 7., 8., 4., 6., 4.,\n        9., 4., 5., 5., 5., 4., 5., 4., 6., 5., 1., 5., 1., 7.],\n       device='cuda:0')\nY = tensor([ 82.,  82.,  52.,  32.,  52.,  42.,  82., 102.,  62.,  82.,  52.,  22.,\n         52.,  32.,  42.,  72., 102.,  42.,  32.,  42.,  42.,  52.,  52.,  22.,\n         32.,  62.,  72.,  52.,  22., 102.,  72.,  82.,  92.,  52.,  72.,  52.,\n        102.,  52.,  62.,  62.,  62.,  52.,  62.,  52.,  72.,  62.,  22.,  62.,\n         22.,  82.], device='cuda:0')\nEpoch 0, Loss: 3178.119873046875, m: 1.0099999904632568, c: 1.0099999904632568\nEpoch 1000, Loss: 161.67578125, m: 8.211641311645508, c: 8.237807273864746\nEpoch 2000, Loss: 0.9927195310592651, m: 10.163670539855957, c: 10.314260482788086\nEpoch 3000, Loss: 0.3464347720146179, m: 10.247573852539062, c: 10.632641792297363\nEpoch 4000, Loss: 0.21811223030090332, m: 10.196943283081055, c: 10.915664672851562\nEpoch 5000, Loss: 0.10084571689367294, m: 10.133870124816895, c: 11.262811660766602\nEpoch 6000, Loss: 0.02807878702878952, m: 10.070626258850098, c: 11.611143112182617\nEpoch 7000, Loss: 0.003373071551322937, m: 10.02446460723877, c: 11.865296363830566\nEpoch 8000, Loss: 0.00010001012560678646, m: 10.004212379455566, c: 11.976828575134277\nEpoch 9000, Loss: 3.049382826247893e-07, m: 10.000231742858887, c: 11.99871826171875\nFinal Loss: 2.937449616879917e-09\nFinal m: 10.00002384185791\nFinal c: 11.999876976013184\n","output_type":"stream"}],"execution_count":20},{"id":"132534a5-543b-4159-901e-cb7f27961314","cell_type":"code","source":"class quadratic_test(torch.nn.Module):\n    def __init__(self):\n        super(quadratic_test,self).__init__()\n        # Parameters\n        self.a = torch.nn.Parameter(data=torch.tensor([1.0], dtype=torch.float32, requires_grad=True))\n        self.b = torch.nn.Parameter(data=torch.tensor([1.0], dtype=torch.float32, requires_grad=True))\n        self.c = torch.nn.Parameter(data=torch.tensor([1.0], dtype=torch.float32, requires_grad=True))\n\n    def forward(self,input):\n        return self.a*input**2 + self.b*input + self.c\n\n# Form training data\na = 1\nb = -4\nc = 3\nX = torch.randint(1, 10, (50,), dtype=torch.float32).to(device)\nY = a*X**2 + b*X + c\n\nmodel = quadratic_test().to(device)\nloss_f = torch.nn.MSELoss()\noptimizer = Adam(model.parameters(), lr=1e-2)\n\nepochs = 10000\n\nfor epoch in range(epochs):\n    optimizer.zero_grad()\n    y_pred = model(X)\n\n    loss = loss_f(y_pred, Y)\n    loss.backward()\n    optimizer.step()\n\n    if epoch % 1000 == 0:\n        print(f\"Epoch {epoch}, Loss: {loss.item()}, a: {model.a.item()}, b: {model.b.item()}, c: {model.c.item()}\")\n\nprint(f\"Final Loss: {loss.item()}\")\nprint(f\"Final a: {model.a.item()}\")\nprint(f\"Final b: {model.b.item()}\")\nprint(f\"Final c: {model.c.item()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:14:39.020076Z","iopub.execute_input":"2025-06-05T15:14:39.020349Z","iopub.status.idle":"2025-06-05T15:14:47.355239Z","shell.execute_reply.started":"2025-06-05T15:14:39.020330Z","shell.execute_reply":"2025-06-05T15:14:47.354589Z"}},"outputs":[{"name":"stdout","text":"Epoch 0, Loss: 735.7999877929688, a: 0.9900000095367432, b: 0.9900000095367432, c: 0.9900000095367432\nEpoch 1000, Loss: 3.6948978900909424, a: 0.67720627784729, b: -0.7560951113700867, c: -2.3576982021331787\nEpoch 2000, Loss: 1.733394742012024, a: 0.7702591419219971, b: -1.4239493608474731, c: -2.729172706604004\nEpoch 3000, Loss: 1.1752963066101074, a: 0.8129366040229797, b: -1.890032172203064, c: -1.7541855573654175\nEpoch 4000, Loss: 0.6107936501502991, a: 0.8652052283287048, b: -2.479386806488037, c: -0.42726579308509827\nEpoch 5000, Loss: 0.20618192851543427, a: 0.921726644039154, b: -3.1168816089630127, c: 1.0089696645736694\nEpoch 6000, Loss: 0.034272484481334686, a: 0.9681078195571899, b: -3.6401431560516357, c: 2.1885225772857666\nEpoch 7000, Loss: 0.0017528521129861474, a: 0.9927934408187866, b: -3.9186818599700928, c: 2.816615581512451\nEpoch 8000, Loss: 1.239864832314197e-05, a: 0.9993945956230164, b: -3.993169069290161, c: 2.9845948219299316\nEpoch 9000, Loss: 3.1425395619066876e-09, a: 0.9999904036521912, b: -3.999891996383667, c: 2.9997570514678955\nFinal Loss: 3.3116974874208838e-12\nFinal a: 0.9999997615814209\nFinal b: -3.999998092651367\nFinal c: 2.9999959468841553\n","output_type":"stream"}],"execution_count":23},{"id":"57138f63-7c88-4bdd-9b04-37dac0a71580","cell_type":"code","source":"# Test optimiser with complex numbers","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}